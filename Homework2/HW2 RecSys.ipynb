{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36ac733",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "In recommender systems, we either assume:\n",
    "\n",
    "- that we know the **rating** that some user gave to some item (for example, \"Joseph Marchand gave 4 stars over 5 to the Suzume movie\"), this is called *explicit feedback*\n",
    "- that we only observe the user interacting with items in a sequence (for example, which songs are played on Spotify or YouTube in which order), but **no ratings**; this is called *implicit feedback*.\n",
    "\n",
    "In this homework, you will build models that optimize either the first setting or the second one.\n",
    "\n",
    "First, execute all cells to ensure you have the necessary packages.\n",
    "\n",
    "## Part 1: Explicit feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0da184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scikit-learn torch spotlight tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52deba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spotlight  # Used as baseline and for preparing datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.evaluation import rmse_score\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6535cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (944 users x 1683 items x 100000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "dataset = get_movielens_dataset(variant='100K')\n",
    "print(dataset)\n",
    "\n",
    "train, test = random_train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a36697d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_users': 944,\n",
       " 'num_items': 1683,\n",
       " 'user_ids': array([642, 796, 557, ...,  79, 606, 201], dtype=int32),\n",
       " 'item_ids': array([832, 226, 180, ..., 276, 288, 603], dtype=int32),\n",
       " 'ratings': array([3., 3., 5., ..., 3., 4., 4.], dtype=float32),\n",
       " 'timestamps': array([892240991, 893048410, 881179653, ..., 891271957, 877641931,\n",
       "        884113924], dtype=int32),\n",
       " 'weights': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7d9e9",
   "metadata": {},
   "source": [
    "Users are numbered from 1 to 943 and items are numbered from 1 to 1682.  \n",
    "Number of users is set to 944 and number of items is set to 1683 to avoid off-by-one errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7752203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an example, user 642 gave 3.0 stars to item 832.\n"
     ]
    }
   ],
   "source": [
    "print(f\"As an example, user {train.user_ids[0]} gave {train.ratings[0]} stars to item {train.item_ids[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7cb95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19820d7",
   "metadata": {},
   "source": [
    "Train / test is a 80:20 split. We should predict in the test set what is the rating for an unseen (user, item) pair: it is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72a838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 131 ms, total: 16.9 s\n",
      "Wall time: 5.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9924273"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = ExplicitFactorizationModel(n_iter=3)\n",
    "model.fit(train)\n",
    "\n",
    "rmse_score(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fb13e",
   "metadata": {},
   "source": [
    "A first exercise is to reproduce this metric. We will do it together in order to make sure that we talk about the same thing, i.e.:\n",
    "\n",
    "$$\\text{RMSE}(y^*, y) = \\sqrt{\\frac1N \\sum_{i = 1}^N (y^*_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51310dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_rmse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d2feed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.544159 , 3.9084182, 3.9396346, ..., 3.5382147, 3.7397728,\n",
       "       3.6165702], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test.user_ids, test.item_ids)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826d1967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[642, 832],\n",
       "         [796, 226],\n",
       "         [557, 180],\n",
       "         [347, 318],\n",
       "         [895, 742]]),\n",
       " tensor([3., 3., 5., 3., 4.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.LongTensor(np.column_stack((train.user_ids, train.item_ids)))\n",
    "X_test = torch.LongTensor(np.column_stack((test.user_ids, test.item_ids)))\n",
    "y_train = torch.Tensor(train.ratings)\n",
    "y_test = torch.Tensor(test.ratings)\n",
    "\n",
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4870483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9924)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3f5ff",
   "metadata": {},
   "source": [
    "> *Yes, I got the same thing.*\n",
    "\n",
    "â€” The Social Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18aca4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1218)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_rmse(y_test, torch.ones_like(y_test) * y_train.mean())  # Simplest baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30f2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "943f3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Recommender system for explicit feedback\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_users, nb_items, embedding_size):\n",
    "        super().__init__()\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "model = CollaborativeFilteringModel(train.num_users, train.num_items, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "loss_function = nn.MSELoss()  # It's a regression problem\n",
    "# You can also check what happens where there is no weight decay i.e. no L2 regularization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    # Your code here, write a training loop and plot train and test RMSE.\n",
    "    # Don't forget that the loss is the mean squared error.\n",
    "    # If you want to display RMSE, you need to take its square root.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef1776",
   "metadata": {},
   "source": [
    "## Part 2: Implicit feedback\n",
    "\n",
    "In this section, we do not observe numerical ratings anymore, just sequences of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1353156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 15.7 ms, total: 22.9 s\n",
      "Wall time: 5.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.041603751419532216"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from spotlight.cross_validation import user_based_train_test_split\n",
    "from spotlight.evaluation import sequence_mrr_score\n",
    "from spotlight.sequence.implicit import ImplicitSequenceModel\n",
    "\n",
    "# If you want to debug on a smaller dataset first, you can use this\n",
    "'''from spotlight.datasets.synthetic import generate_sequential\n",
    "dataset = generate_sequential(num_users=100,\n",
    "                              num_items=1000,\n",
    "                              num_interactions=10000,\n",
    "                              concentration_parameter=0.01,\n",
    "                              order=3)'''\n",
    "\n",
    "# Otherwise we reuse Movielens\n",
    "train, test = user_based_train_test_split(dataset)\n",
    "\n",
    "train = train.to_sequence()\n",
    "test = test.to_sequence()\n",
    "\n",
    "model = ImplicitSequenceModel(n_iter=3,\n",
    "                              representation='pooling',\n",
    "                              loss='pointwise')\n",
    "model.fit(train)\n",
    "\n",
    "sequence_mrr_score(model, test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54b1ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequence interactions dataset (8249 sequences x 10 sequence length)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9baa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequences': array([[ 209,   32,  189, ...,    5,   74,  102],\n",
       "        [ 255,  272,  271, ...,  244,   18,  270],\n",
       "        [ 100,  154,    9, ...,  222,  258,  266],\n",
       "        ...,\n",
       "        [ 928,   24,  274, ..., 1047,  111,  284],\n",
       "        [ 763,   50,  412, ...,  685,  471,  405],\n",
       "        [   0,    0,   64, ..., 1067,  127,  508]], dtype=int32),\n",
       " 'user_ids': array([  1,   1,   1, ..., 943, 943, 943], dtype=int32),\n",
       " 'max_sequence_length': 10,\n",
       " 'num_items': 1683}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f57dc",
   "metadata": {},
   "source": [
    "The `train` dataset contains 8000+ sequences of length 10 representing the movies seen, IDs between 1 and 1683. \n",
    "\n",
    "We do have access to user IDs but we will not need them. Here, the maximum length of a sequence is 10, so sequences have been split to be of max size 10. Sequences having less than 10 items are padded with 0s.\n",
    "\n",
    "The objective becomes, given the first 9 items, predict the 10th item (classification). In order to have a better comparison of models, we are mainly interested in a ranking metric: in the ranked movies by probability, where was the correct answer? Mean reciprocal rank is, for $N$ samples:\n",
    "\n",
    "$${\\text{MRR}}={\\frac {1}{N}}\\sum _{{i=1}}^{{N}}{\\frac {1}{{\\text{rank}}_{i}}}$$\n",
    "\n",
    "where $\\text{rank}_i$ is a number between 1 and 1683, the number of items, which represents the rank of the expected answer, where movies are ranked by decreasing probability. The MRR is between 0 and 1 and higher is better (when all ranks are 1).\n",
    "\n",
    "Again, we will first attempt to reproduce the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba62e759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2190, 1683)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "ohe = OneHotEncoder(categories=[list(range(train.num_items))])\n",
    "target = ohe.fit_transform(test.sequences[:, [-1]]).toarray()\n",
    "\n",
    "target.shape  # The target is a one-hot encoding of correct answers for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4a77620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.01, 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.01, 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.03, 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "for seq in test.sequences:\n",
    "    y_pred.append(softmax(model.predict(seq[:-1])))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.round(2)  # The predictions are probabilities for each sample x item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0fe2a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04160375141953235"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function is actually more generic as it can also consider multilabel classification\n",
    "# i.e. several correct answers for a sample. In our case (1 correct answer per sample) it is equal to MRR.\n",
    "label_ranking_average_precision_score(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b96e8",
   "metadata": {},
   "source": [
    "Again, we got the same number. We now show the metric for two simple baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e352b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007895772118173742"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "item_counts = Counter(train.sequences[:, -1])\n",
    "most_popular_item = item_counts.most_common()[0][0]\n",
    "most_popular_baseline = np.zeros_like(target)\n",
    "most_popular_baseline[:, most_popular_item] = 1\n",
    "label_ranking_average_precision_score(target, most_popular_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72df46d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024785953467286397"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity = np.zeros(train.num_items)\n",
    "for item_id, count in item_counts.items():\n",
    "    popularity[item_id] = count\n",
    "popularity = softmax(popularity)\n",
    "popularity_baseline = np.tile(popularity, (len(target), 1))  # Repeat for each test sample\n",
    "label_ranking_average_precision_score(target, popularity_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718098d",
   "metadata": {},
   "source": [
    "You should now write a model / module that takes a batch of sequences of max 9 elements and should predict the next one.\n",
    "\n",
    "You can either take:\n",
    "- a sequential approach, i.e. RNN / LSTM / GRU or a transformer like [minGPT](https://github.com/karpathy/minGPT) (which is more complex; please start simple);\n",
    "- or a non-sequential one like [CBOW](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#context-based-continuous-bag-of-words-cbow).\n",
    "\n",
    "Loss can either be cross-entropy (simple), [noise contrastive estimation, or negative sampling](https://lilianweng.github.io/posts/2017-10-15-word-embedding/#noise-contrastive-estimation-nce).\n",
    "\n",
    "The goal is to have comparable or better MRR than 0.04.\n",
    "\n",
    "P. S. â€“ In order to ignore index 0 you can use `padding_idx` in [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) or the function [`masked_select()`](https://pytorch.org/docs/stable/generated/torch.masked_select.html) for RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
